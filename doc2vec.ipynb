{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim\n",
    "import gensim\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "TaggedDocument = gensim.models.doc2vec.TaggedDocument\n",
    "import re\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up logging configurations  \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lore.txt', 'news.txt', 'reviews.txt', 'romance.txt', 'hobbies.txt', 'humor.txt', 'learned.txt', 'science_fiction.txt', 'belles_lettres.txt', 'mystery.txt', 'government.txt', 'religion.txt', 'editorial.txt', 'adventure.txt', 'fiction.txt']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "docLabels = []\n",
    "docLabels = [f for f in listdir(\"/home/fatemeh/Documents/python/dnn-understanding/code/text\") if f.endswith('.txt')]\n",
    "print(docLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in docLabels:\n",
    "    data.append(open(\"/home/fatemeh/Documents/python/dnn-understanding/code/text/\" + doc, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):    \n",
    "            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = LabeledLineSentence(data, docLabels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-26 18:48:31,784 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-344-67bc8426a6f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use fixed learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fatemeh/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fatemeh/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, progress_per, trim_rule, update)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-342-87b4351521aa>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Doc2Vec(size=300, window=10, min_count=5, workers=11,alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "\n",
    "model.build_vocab(iter)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train(it)\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no deca\n",
    "    model.train(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-26 16:25:51,559 : INFO : collecting all words and their counts\n",
      "2017-05-26 16:25:51,562 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-05-26 16:25:51,564 : INFO : collected 5 word types and 2 unique tags from a corpus of 2 examples and 6 words\n",
      "2017-05-26 16:25:51,568 : INFO : Loading a fresh vocabulary\n",
      "2017-05-26 16:25:51,574 : INFO : min_count=1 retains 5 unique words (100% of original 5, drops 0)\n",
      "2017-05-26 16:25:51,575 : INFO : min_count=1 leaves 6 word corpus (100% of original 6, drops 0)\n",
      "2017-05-26 16:25:51,577 : INFO : deleting the raw counts dictionary of 5 items\n",
      "2017-05-26 16:25:51,579 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2017-05-26 16:25:51,583 : INFO : downsampling leaves estimated 0 word corpus (7.5% of prior 6)\n",
      "2017-05-26 16:25:51,585 : INFO : estimated required memory for 5 words and 100 dimensions: 7700 bytes\n",
      "2017-05-26 16:25:51,587 : INFO : resetting layer weights\n",
      "2017-05-26 16:25:51,589 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,592 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,597 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,599 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,603 : INFO : training on 30 raw words (11 effective words) took 0.0s, 1628 effective words/s\n",
      "2017-05-26 16:25:51,605 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,606 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,608 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,614 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,615 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,617 : INFO : training on 30 raw words (15 effective words) took 0.0s, 2489 effective words/s\n",
      "2017-05-26 16:25:51,618 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,620 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,621 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,626 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,630 : INFO : training on 30 raw words (10 effective words) took 0.0s, 2002 effective words/s\n",
      "2017-05-26 16:25:51,631 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,632 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,634 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,642 : INFO : training on 30 raw words (14 effective words) took 0.0s, 2743 effective words/s\n",
      "2017-05-26 16:25:51,643 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,645 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,646 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,654 : INFO : training on 30 raw words (12 effective words) took 0.0s, 2360 effective words/s\n",
      "2017-05-26 16:25:51,655 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,657 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,658 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,662 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,663 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,666 : INFO : training on 30 raw words (10 effective words) took 0.0s, 1998 effective words/s\n",
      "2017-05-26 16:25:51,667 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,669 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,670 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,679 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,680 : INFO : training on 30 raw words (10 effective words) took 0.0s, 1878 effective words/s\n",
      "2017-05-26 16:25:51,682 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,683 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,684 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,689 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,691 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,693 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,694 : INFO : training on 30 raw words (12 effective words) took 0.0s, 2283 effective words/s\n",
      "2017-05-26 16:25:51,696 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,698 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,699 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-26 16:25:51,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,708 : INFO : training on 30 raw words (10 effective words) took 0.0s, 1931 effective words/s\n",
      "2017-05-26 16:25:51,709 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,710 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-26 16:25:51,712 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-26 16:25:51,715 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-26 16:25:51,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-26 16:25:51,718 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-26 16:25:51,719 : INFO : training on 30 raw words (11 effective words) took 0.0s, 2251 effective words/s\n",
      "2017-05-26 16:25:51,720 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-05-26 16:25:51,722 : INFO : saving Doc2Vec object under my_model.doc2vec, separately None\n",
      "2017-05-26 16:25:51,723 : INFO : not storing attribute syn0norm\n",
      "2017-05-26 16:25:51,724 : INFO : not storing attribute cum_table\n",
      "2017-05-26 16:25:51,726 : INFO : saved my_model.doc2vec\n",
      "2017-05-26 16:25:51,728 : INFO : loading Doc2Vec object from my_model.doc2vec\n",
      "2017-05-26 16:25:51,729 : INFO : loading wv recursively from my_model.doc2vec.wv.* with mmap=None\n",
      "2017-05-26 16:25:51,731 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-26 16:25:51,732 : INFO : loading docvecs recursively from my_model.doc2vec.docvecs.* with mmap=None\n",
      "2017-05-26 16:25:51,733 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-26 16:25:51,734 : INFO : loaded my_model.doc2vec\n",
      "2017-05-26 16:25:51,735 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2017-05-26 16:25:51,737 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SENT_1', 0.07189338654279709)]\n",
      "[('SENT_0', 0.07189340144395828)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "sentence = models.doc2vec.LabeledSentence(\n",
    "    words=[u'so`bme', u'words', u'here'], tags=[\"SENT_0\"])\n",
    "\n",
    "sentence1 = models.doc2vec.LabeledSentence(\n",
    "    words=[u'here', u'we', u'go'], tags=[\"SENT_1\"])\n",
    "\n",
    "sentences = [sentence, sentence1]\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def __iter__(self):\n",
    "        for uid, line in enumerate(open(filename)):\n",
    "            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])\n",
    "            \n",
    "model = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1)\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train(sentences)\n",
    "    model.alpha -= 0.002  # decrease the learning rate`\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "model.save(\"my_model.doc2vec\")\n",
    "model_loaded = models.Doc2Vec.load('my_model.doc2vec')\n",
    "\n",
    "print (model.docvecs.most_similar([\"SENT_0\"]))\n",
    "print (model_loaded.docvecs.most_similar([\"SENT_1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LabeledSentence is simply a tidier way to do that. It contains a list of words, and a label for the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ada28018892e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# you can continue training with the loaded model!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#save model \n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-963c3556ad6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"This is a sentence\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"This is another sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc2vec' is not defined"
     ]
    }
   ],
   "source": [
    "doc=[\"This is a sentence\",\"This is another sentence\"]\n",
    "documents=[doc.strip().split(\" \") for doc in doc ]\n",
    "model = doc2vec.Doc2Vec(documents, size = 100, window = 300, min_count = 10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
